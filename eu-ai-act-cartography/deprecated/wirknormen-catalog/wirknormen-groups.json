{
  "metadata": {
    "document": "EU AI Act (Regulation (EU) 2024/1689)",
    "analysis_date": "2025-10-09",
    "methodology": "Systematic analysis of all articles to identify legal consequences (Wirknormen) - provisions that create obligations, prohibitions, or classify systems",
    "total_wirknorm_groups": 45,
    "total_individual_wirknormen": 178
  },

  "wirknorm_groups": [
    {
      "group_id": "WG-001",
      "group_name": "Prohibited AI Practices - Manipulative Systems",
      "shared_requirements": [
        "Is an AI system",
        "System is placed on market, put into service, or used",
        "In scope of Regulation (not excluded by Art 2)"
      ],
      "legal_consequences": [
        {
          "article": "Article 5(1)(a)",
          "description": "Prohibition of AI systems deploying subliminal techniques or manipulative/deceptive techniques",
          "specific_condition": "System has objective or effect of materially distorting behaviour by impairing ability to make informed decision, causing significant harm"
        },
        {
          "article": "Article 5(1)(b)",
          "description": "Prohibition of AI systems exploiting vulnerabilities of persons due to age, disability, or social/economic situation",
          "specific_condition": "System has objective or effect of materially distorting behaviour in manner causing significant harm"
        }
      ],
      "notes": "These prohibitions share identical structural requirements but differ in the specific manipulative technique employed"
    },

    {
      "group_id": "WG-002",
      "group_name": "Prohibited AI Practices - Social Scoring",
      "shared_requirements": [
        "Is an AI system",
        "System is placed on market, put into service, or used",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 5(1)(c)(i)",
          "description": "Prohibition of social scoring leading to detrimental treatment unrelated to original context",
          "specific_condition": "Treatment in social contexts unrelated to where data was generated/collected"
        },
        {
          "article": "Article 5(1)(c)(ii)",
          "description": "Prohibition of social scoring leading to unjustified/disproportionate treatment",
          "specific_condition": "Treatment is unjustified or disproportionate to social behaviour or its gravity"
        }
      ],
      "notes": "Both prohibitions target social scoring but differ in the nature of the detrimental treatment"
    },

    {
      "group_id": "WG-003",
      "group_name": "Prohibited AI Practices - Predictive Risk Assessment",
      "shared_requirements": [
        "Is an AI system",
        "System is placed on market, put into service for specific purpose, or used",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 5(1)(d)",
          "description": "Prohibition of AI systems making risk assessments of natural persons for criminal offence prediction",
          "specific_condition": "Assessment is based solely on profiling or assessing personality traits/characteristics; exception for systems supporting human assessment based on objective verifiable facts"
        }
      ],
      "notes": "Narrow prohibition with specific exception clause"
    },

    {
      "group_id": "WG-004",
      "group_name": "Prohibited AI Practices - Facial Recognition Database Creation",
      "shared_requirements": [
        "Is an AI system",
        "System is placed on market, put into service for specific purpose, or used",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 5(1)(e)",
          "description": "Prohibition of AI systems creating/expanding facial recognition databases through untargeted scraping",
          "specific_condition": "Scraping from internet or CCTV footage"
        }
      ],
      "notes": "Absolute prohibition with no exceptions"
    },

    {
      "group_id": "WG-005",
      "group_name": "Prohibited AI Practices - Emotion Inference",
      "shared_requirements": [
        "Is an AI system",
        "System is placed on market, put into service for specific purpose, or used",
        "System infers emotions in workplace or education",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 5(1)(f)",
          "description": "Prohibition of AI systems inferring emotions in workplace/education",
          "specific_condition": "Exception for medical or safety reasons"
        }
      ],
      "notes": "Context-specific prohibition with narrow exception"
    },

    {
      "group_id": "WG-006",
      "group_name": "Prohibited AI Practices - Biometric Categorisation",
      "shared_requirements": [
        "Is an AI system",
        "System is biometric categorisation system",
        "System is placed on market, put into service for specific purpose, or used",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 5(1)(g)",
          "description": "Prohibition of biometric categorisation to deduce race, political opinions, trade union membership, religious/philosophical beliefs, sex life, or sexual orientation",
          "specific_condition": "Categorises individually based on biometric data; does not cover lawful labelling/filtering or law enforcement categorisation"
        }
      ],
      "notes": "Targets sensitive attribute inference"
    },

    {
      "group_id": "WG-007",
      "group_name": "Restricted AI Practice - Real-time Remote Biometric Identification",
      "shared_requirements": [
        "Is real-time remote biometric identification system",
        "Use in publicly accessible space",
        "For law enforcement purposes",
        "In scope of Regulation",
        "Member State has provided for possibility in national law (Art 5(5))"
      ],
      "legal_consequences": [
        {
          "article": "Article 5(1)(h)(i)",
          "description": "Conditionally permitted use for targeted search of abduction/trafficking victims or missing persons",
          "specific_condition": "Subject to prior authorisation (Art 5(3)), fundamental rights impact assessment, registration, proportionality safeguards"
        },
        {
          "article": "Article 5(1)(h)(ii)",
          "description": "Conditionally permitted use for prevention of specific, substantial and imminent threat to life/safety or terrorist attack",
          "specific_condition": "Subject to prior authorisation (Art 5(3)), fundamental rights impact assessment, registration, proportionality safeguards"
        },
        {
          "article": "Article 5(1)(h)(iii)",
          "description": "Conditionally permitted use for localisation/identification of perpetrators/suspects of serious criminal offences",
          "specific_condition": "Offences listed in Annex II, punishable by custodial sentence/detention of at least 4 years; subject to prior authorisation (Art 5(3)), fundamental rights impact assessment, registration, proportionality safeguards"
        }
      ],
      "notes": "Complex multi-layered requirements including authorisation by judicial/independent administrative authority, urgency exceptions (24h notification), market surveillance notification (Art 5(4)), annual reporting (Art 5(6))"
    },

    {
      "group_id": "WG-008",
      "group_name": "High-Risk AI System Classification - Safety Component",
      "shared_requirements": [
        "Is an AI system",
        "System is safety component of product OR is itself a product",
        "Product covered by Union harmonisation legislation in Annex I",
        "Product requires third-party conformity assessment under that legislation",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 6(1)",
          "description": "System is classified as high-risk AI system",
          "specific_condition": "Both conditions (safety component + third-party assessment requirement) must be fulfilled"
        }
      ],
      "notes": "First pathway to high-risk classification"
    },

    {
      "group_id": "WG-009",
      "group_name": "High-Risk AI System Classification - Annex III Listed Systems",
      "shared_requirements": [
        "Is an AI system",
        "System falls within use-cases listed in Annex III",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 6(2) with Annex III, point 1(a)",
          "description": "Remote biometric identification systems (not verification) are classified as high-risk",
          "specific_condition": "System used for identification, not mere verification of claimed identity"
        },
        {
          "article": "Article 6(2) with Annex III, point 1(b)",
          "description": "Biometric categorisation systems are classified as high-risk",
          "specific_condition": "Categorisation based on sensitive/protected attributes"
        },
        {
          "article": "Article 6(2) with Annex III, point 1(c)",
          "description": "Emotion recognition systems are classified as high-risk",
          "specific_condition": "System used for emotion recognition"
        },
        {
          "article": "Article 6(2) with Annex III, point 2",
          "description": "Critical infrastructure safety AI systems are classified as high-risk",
          "specific_condition": "Safety components for critical digital infrastructure, road traffic, or water/gas/heating/electricity supply"
        },
        {
          "article": "Article 6(2) with Annex III, point 3(a)",
          "description": "Education access/admission AI systems are classified as high-risk",
          "specific_condition": "Determines access/admission or assigns persons to educational institutions at all levels"
        },
        {
          "article": "Article 6(2) with Annex III, point 3(b)",
          "description": "Learning outcome evaluation AI systems are classified as high-risk",
          "specific_condition": "Evaluates learning outcomes, including when used to steer learning process"
        },
        {
          "article": "Article 6(2) with Annex III, point 3(c)",
          "description": "Education level assessment AI systems are classified as high-risk",
          "specific_condition": "Assesses appropriate education level individual will receive or access"
        },
        {
          "article": "Article 6(2) with Annex III, point 3(d)",
          "description": "Student monitoring AI systems are classified as high-risk",
          "specific_condition": "Monitors/detects prohibited behaviour during tests"
        },
        {
          "article": "Article 6(2) with Annex III, point 4(a)",
          "description": "Recruitment/selection AI systems are classified as high-risk",
          "specific_condition": "Used for targeted job ads, analysing/filtering applications, evaluating candidates"
        },
        {
          "article": "Article 6(2) with Annex III, point 4(b)",
          "description": "Employment relationship AI systems are classified as high-risk",
          "specific_condition": "Makes decisions on work terms, promotion, termination, task allocation, or monitoring/evaluation"
        },
        {
          "article": "Article 6(2) with Annex III, point 5(a)",
          "description": "Public assistance eligibility AI systems are classified as high-risk",
          "specific_condition": "Evaluates eligibility for essential public benefits/services or grants/reduces/revokes/reclaims them"
        },
        {
          "article": "Article 6(2) with Annex III, point 5(b)",
          "description": "Creditworthiness evaluation AI systems are classified as high-risk",
          "specific_condition": "Evaluates creditworthiness or establishes credit score; exception for fraud detection"
        },
        {
          "article": "Article 6(2) with Annex III, point 5(c)",
          "description": "Life/health insurance risk assessment AI systems are classified as high-risk",
          "specific_condition": "Risk assessment and pricing for natural persons"
        },
        {
          "article": "Article 6(2) with Annex III, point 5(d)",
          "description": "Emergency call evaluation/dispatch AI systems are classified as high-risk",
          "specific_condition": "Evaluates/classifies emergency calls or dispatches/prioritises emergency services, including triage systems"
        },
        {
          "article": "Article 6(2) with Annex III, point 6(a)",
          "description": "Law enforcement victim risk assessment AI systems are classified as high-risk",
          "specific_condition": "Assesses risk of person becoming crime victim"
        },
        {
          "article": "Article 6(2) with Annex III, point 6(b)",
          "description": "Law enforcement polygraph AI systems are classified as high-risk",
          "specific_condition": "Used as polygraphs or similar tools"
        },
        {
          "article": "Article 6(2) with Annex III, point 6(c)",
          "description": "Law enforcement evidence reliability AI systems are classified as high-risk",
          "specific_condition": "Evaluates reliability of evidence in investigation/prosecution"
        },
        {
          "article": "Article 6(2) with Annex III, point 6(d)",
          "description": "Law enforcement offending risk assessment AI systems are classified as high-risk",
          "specific_condition": "Assesses risk of offending/reoffending not solely on profiling, or assesses personality traits/past criminal behaviour"
        },
        {
          "article": "Article 6(2) with Annex III, point 6(e)",
          "description": "Law enforcement profiling AI systems are classified as high-risk",
          "specific_condition": "Profiling in detection/investigation/prosecution of criminal offences"
        },
        {
          "article": "Article 6(2) with Annex III, point 7(a)",
          "description": "Migration/asylum/border control polygraph AI systems are classified as high-risk",
          "specific_condition": "Used as polygraphs or similar tools"
        },
        {
          "article": "Article 6(2) with Annex III, point 7(b)",
          "description": "Migration/asylum/border control risk assessment AI systems are classified as high-risk",
          "specific_condition": "Assesses security risk, irregular migration risk, or health risk of persons entering/in territory"
        },
        {
          "article": "Article 6(2) with Annex III, point 7(c)",
          "description": "Asylum/visa/residence permit examination AI systems are classified as high-risk",
          "specific_condition": "Assists examination of applications/complaints regarding eligibility, including evidence reliability assessment"
        },
        {
          "article": "Article 6(2) with Annex III, point 7(d)",
          "description": "Migration/asylum/border control identification AI systems are classified as high-risk",
          "specific_condition": "Detects/recognises/identifies persons; exception for travel document verification"
        },
        {
          "article": "Article 6(2) with Annex III, point 8(a)",
          "description": "Judicial assistance AI systems are classified as high-risk",
          "specific_condition": "Assists judicial authority in researching/interpreting facts and law, or used similarly in alternative dispute resolution"
        },
        {
          "article": "Article 6(2) with Annex III, point 8(b)",
          "description": "Election/referendum influence AI systems are classified as high-risk",
          "specific_condition": "Intended to influence election/referendum outcome or voting behaviour; exception for administrative/logistical campaign tools"
        }
      ],
      "notes": "Second pathway to high-risk classification. Subject to derogation under Art 6(3) if system does not materially influence outcome or meets specific conditions (narrow procedural task, improvement of prior human activity, pattern detection, preparatory task). Exception does not apply if system performs profiling. Provider must document assessment (Art 6(4)) and register system (Art 49(2))."
    },

    {
      "group_id": "WG-010",
      "group_name": "High-Risk AI System Requirements - Risk Management",
      "shared_requirements": [
        "Is high-risk AI system (under Art 6)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 9(1)",
          "description": "Provider must establish, implement, document and maintain risk management system",
          "specific_condition": "Continuous iterative process throughout lifecycle"
        }
      ],
      "notes": "Risk management system must include: (a) identification/analysis of known and foreseeable risks; (b) estimation/evaluation of risks under intended purpose and foreseeable misuse; (c) evaluation of risks from post-market monitoring data; (d) adoption of appropriate risk management measures. Special consideration required for impact on persons under 18 and vulnerable groups (Art 9(9))."
    },

    {
      "group_id": "WG-011",
      "group_name": "High-Risk AI System Requirements - Data Governance",
      "shared_requirements": [
        "Is high-risk AI system (under Art 6)",
        "System uses training of AI models with data",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 10(1)-(4)",
          "description": "Provider must ensure training/validation/testing datasets meet quality criteria",
          "specific_condition": "Datasets must be relevant, sufficiently representative, free of errors, complete, have appropriate statistical properties, take into account geographical/contextual/behavioural/functional setting"
        },
        {
          "article": "Article 10(5)",
          "description": "Provider may exceptionally process special categories of personal data for bias detection/correction",
          "specific_condition": "Subject to six cumulative conditions: (a) cannot be fulfilled by other data; (b) technical limitations on re-use; (c) access controls and documentation; (d) no transmission to other parties; (e) deletion after correction or retention period; (f) records explaining necessity"
        }
      ],
      "notes": "Article 10(2) specifies data governance practices must concern: design choices, collection processes, data preparation operations, formulation of assumptions, availability/quantity/suitability assessment, bias examination, bias mitigation measures, identification of data gaps."
    },

    {
      "group_id": "WG-012",
      "group_name": "High-Risk AI System Requirements - Technical Documentation",
      "shared_requirements": [
        "Is high-risk AI system (under Art 6)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 11(1)",
          "description": "Provider must draw up and keep up-to-date technical documentation before placing on market/putting into service",
          "specific_condition": "Must contain minimum elements in Annex IV; SMEs may use simplified form provided by Commission"
        }
      ],
      "notes": "Documentation must demonstrate compliance with Section 2 requirements. For products covered by Union harmonisation legislation in Annex I Section A, single set of documentation required (Art 11(2))."
    },

    {
      "group_id": "WG-013",
      "group_name": "High-Risk AI System Requirements - Record-keeping (Logs)",
      "shared_requirements": [
        "Is high-risk AI system (under Art 6)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 12(1)-(2)",
          "description": "Provider must ensure system technically allows automatic recording of events (logs)",
          "specific_condition": "Logs must enable: (a) identifying situations presenting risk or substantial modification; (b) facilitating post-market monitoring; (c) monitoring operation of Annex III point 1(a) systems"
        },
        {
          "article": "Article 12(3)",
          "description": "For remote biometric identification systems (Annex III point 1(a)), specific logging requirements apply",
          "specific_condition": "Must record: (a) period of each use; (b) reference database; (c) input data with match; (d) identification of persons verifying results per Art 14(5)"
        }
      ],
      "notes": "Logging capabilities must be appropriate to intended purpose"
    },

    {
      "group_id": "WG-014",
      "group_name": "High-Risk AI System Requirements - Transparency and Instructions",
      "shared_requirements": [
        "Is high-risk AI system (under Art 6)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 13(1)-(3)",
          "description": "Provider must design system with sufficient transparency and provide instructions for use",
          "specific_condition": "Instructions must include: provider identity/contact, intended purpose, accuracy/robustness/cybersecurity levels, known circumstances leading to risks, technical capabilities for explaining output, performance for specific persons/groups, input data specifications, output interpretation information, predetermined changes, human oversight measures, computational/hardware resources, maintenance measures, logging mechanisms"
        }
      ],
      "notes": "Instructions must be concise, complete, correct, clear, relevant, accessible and comprehensible"
    },

    {
      "group_id": "WG-015",
      "group_name": "High-Risk AI System Requirements - Human Oversight",
      "shared_requirements": [
        "Is high-risk AI system (under Art 6)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 14(1)-(4)",
          "description": "Provider must design system to enable effective oversight by natural persons",
          "specific_condition": "Oversight must enable persons to: understand capacities/limitations, remain aware of automation bias, correctly interpret output, decide not to use/disregard/override output, intervene or stop system"
        },
        {
          "article": "Article 14(5)",
          "description": "For remote biometric identification systems (Annex III point 1(a)), enhanced oversight required",
          "specific_condition": "No action/decision based on identification unless verified by at least two natural persons with necessary competence; exception for law enforcement/migration/border control/asylum where Union/national law deems disproportionate"
        }
      ],
      "notes": "Oversight measures must be commensurate with risks, autonomy level and context of use"
    },

    {
      "group_id": "WG-016",
      "group_name": "High-Risk AI System Requirements - Accuracy, Robustness, Cybersecurity",
      "shared_requirements": [
        "Is high-risk AI system (under Art 6)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 15(1)-(5)",
          "description": "Provider must design system to achieve appropriate level of accuracy, robustness and cybersecurity",
          "specific_condition": "Must be resilient to errors/faults/inconsistencies, address AI-specific vulnerabilities (data poisoning, model poisoning, adversarial examples, model evasion, confidentiality attacks), resilient against unauthorised third-party manipulation"
        }
      ],
      "notes": "Accuracy levels and metrics must be declared in instructions. Robustness may be achieved through technical redundancy, backup/fail-safe plans. Systems that continue learning must eliminate/reduce biased output feedback loops."
    },

    {
      "group_id": "WG-017",
      "group_name": "Provider Obligations - General High-Risk AI Systems",
      "shared_requirements": [
        "Is provider of high-risk AI system (under Art 6)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 16(a)",
          "description": "Provider must ensure high-risk AI system complies with Section 2 requirements",
          "specific_condition": "All requirements in Articles 8-15"
        },
        {
          "article": "Article 16(b)",
          "description": "Provider must indicate name, registered trade name/mark, contact address on system/packaging/documentation",
          "specific_condition": "Identification obligation"
        },
        {
          "article": "Article 16(c)",
          "description": "Provider must have quality management system complying with Article 17",
          "specific_condition": "QMS must be in place"
        },
        {
          "article": "Article 16(d)",
          "description": "Provider must keep documentation under Article 18",
          "specific_condition": "10-year retention period"
        },
        {
          "article": "Article 16(e)",
          "description": "Provider must keep automatically generated logs under their control",
          "specific_condition": "Per Article 19, at least 6 months unless otherwise provided"
        },
        {
          "article": "Article 16(f)",
          "description": "Provider must ensure system undergoes relevant conformity assessment procedure",
          "specific_condition": "Per Article 43, prior to placing on market/putting into service"
        },
        {
          "article": "Article 16(g)",
          "description": "Provider must draw up EU declaration of conformity",
          "specific_condition": "Per Article 47"
        },
        {
          "article": "Article 16(h)",
          "description": "Provider must affix CE marking to system/packaging/documentation",
          "specific_condition": "Per Article 48, to indicate conformity"
        },
        {
          "article": "Article 16(i)",
          "description": "Provider must comply with registration obligations",
          "specific_condition": "Per Article 49(1)"
        },
        {
          "article": "Article 16(j)",
          "description": "Provider must take necessary corrective actions and provide information",
          "specific_condition": "Per Article 20, when system not in conformity or presents risk"
        },
        {
          "article": "Article 16(k)",
          "description": "Provider must demonstrate conformity upon reasoned request of national competent authority",
          "specific_condition": "Duty to demonstrate compliance"
        },
        {
          "article": "Article 16(l)",
          "description": "Provider must ensure system complies with accessibility requirements",
          "specific_condition": "Per Directives (EU) 2016/2102 and (EU) 2019/882"
        }
      ],
      "notes": "Comprehensive set of provider obligations for high-risk AI systems"
    },

    {
      "group_id": "WG-018",
      "group_name": "Provider Obligations - Quality Management System",
      "shared_requirements": [
        "Is provider of high-risk AI system (under Art 6)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 17(1)",
          "description": "Provider must put in place documented quality management system covering 13 aspects",
          "specific_condition": "Must include: (a) regulatory compliance strategy; (b) design techniques/procedures; (c) development quality control; (d) examination/test/validation procedures; (e) technical specifications/standards; (f) data management systems; (g) risk management system; (h) post-market monitoring; (i) serious incident reporting; (j) communication handling; (k) record-keeping; (l) resource management; (m) accountability framework"
        },
        {
          "article": "Article 17(4)",
          "description": "Financial institutions subject to Union financial services law may fulfil QMS through existing internal governance",
          "specific_condition": "Except for points (g), (h), (i) which must be separately fulfilled; must take into account harmonised standards"
        }
      ],
      "notes": "Implementation must be proportionate to provider size. May integrate with existing sectoral QMS under Art 17(3)."
    },

    {
      "group_id": "WG-019",
      "group_name": "Provider Obligations - Corrective Actions",
      "shared_requirements": [
        "Is provider of high-risk AI system (under Art 6)",
        "Provider considers or has reason to consider system not in conformity OR system presents risk per Art 79(1)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 20(1)",
          "description": "Provider must immediately take necessary corrective actions to bring into conformity, withdraw, disable, or recall",
          "specific_condition": "Must inform distributors, deployers, authorised representatives, and importers"
        },
        {
          "article": "Article 20(2)",
          "description": "When system presents risk, provider must immediately investigate causes and inform market surveillance authorities and notified body",
          "specific_condition": "Must inform of nature of non-compliance and corrective actions taken; collaboration with deployer if applicable"
        }
      ],
      "notes": "Immediate action required upon discovery of non-conformity or risk"
    },

    {
      "group_id": "WG-020",
      "group_name": "Provider Obligations - Cooperation with Authorities",
      "shared_requirements": [
        "Is provider of high-risk AI system (under Art 6)",
        "Competent authority makes reasoned request",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 21(1)",
          "description": "Provider must provide all information and documentation necessary to demonstrate conformity",
          "specific_condition": "In language easily understood by authority in official EU language indicated by Member State"
        },
        {
          "article": "Article 21(2)",
          "description": "Provider must give access to automatically generated logs under their control",
          "specific_condition": "Upon reasoned request"
        }
      ],
      "notes": "Information treated per confidentiality obligations in Art 78"
    },

    {
      "group_id": "WG-021",
      "group_name": "Authorised Representative Obligations",
      "shared_requirements": [
        "Is provider of high-risk AI system established in third country",
        "Intends to make system available on Union market",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 22(1)",
          "description": "Provider must appoint authorised representative established in Union by written mandate",
          "specific_condition": "Prior to making system available on Union market"
        },
        {
          "article": "Article 22(3)",
          "description": "Authorised representative must perform specified tasks",
          "specific_condition": "Tasks include: (a) verifying EU declaration and technical documentation; (b) keeping documentation for 10 years; (c) providing information to authorities; (d) cooperating with authorities; (e) complying with/ensuring registration obligations"
        },
        {
          "article": "Article 22(4)",
          "description": "Authorised representative must terminate mandate if provider acts contrary to obligations",
          "specific_condition": "Must immediately inform market surveillance authority and notified body of termination and reasons"
        }
      ],
      "notes": "Authorised representative can be addressed by authorities in addition to or instead of provider"
    },

    {
      "group_id": "WG-022",
      "group_name": "Importer Obligations",
      "shared_requirements": [
        "Is importer of high-risk AI system",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 23(1)",
          "description": "Before placing on market, importer must verify system conformity",
          "specific_condition": "Must verify: (a) conformity assessment done; (b) technical documentation drawn up; (c) CE marking and EU declaration present; (d) authorised representative appointed"
        },
        {
          "article": "Article 23(2)",
          "description": "If sufficient reason to consider non-conformity/falsification, importer must not place on market until conformity achieved",
          "specific_condition": "If system presents risk per Art 79(1), must inform provider, authorised representative and market surveillance authorities"
        },
        {
          "article": "Article 23(3)",
          "description": "Importer must indicate name, registered trade name/mark, contact address on system/packaging/documentation",
          "specific_condition": "Identification obligation"
        },
        {
          "article": "Article 23(4)",
          "description": "Importer must ensure storage/transport conditions do not jeopardise Section 2 compliance",
          "specific_condition": "While under their responsibility"
        },
        {
          "article": "Article 23(5)",
          "description": "Importer must keep copy of certificate, instructions, EU declaration for 10 years",
          "specific_condition": "After system placed on market/put into service"
        },
        {
          "article": "Article 23(6)",
          "description": "Importer must provide necessary information/documentation to authorities upon reasoned request",
          "specific_condition": "In easily understood language; ensure technical documentation available"
        },
        {
          "article": "Article 23(7)",
          "description": "Importer must cooperate with authorities in actions regarding system",
          "specific_condition": "To reduce/mitigate risks"
        }
      ],
      "notes": "Comprehensive obligations for importers bringing high-risk AI systems into Union market"
    },

    {
      "group_id": "WG-023",
      "group_name": "Distributor Obligations",
      "shared_requirements": [
        "Is distributor of high-risk AI system",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 24(1)",
          "description": "Before making available, distributor must verify CE marking, EU declaration, instructions present and provider/importer complied with obligations",
          "specific_condition": "Verification of Art 16 points (b)(c) and Art 23(3) compliance"
        },
        {
          "article": "Article 24(2)",
          "description": "If reason to consider non-conformity, distributor must not make available until conformity achieved",
          "specific_condition": "If system presents risk per Art 79(1), must inform provider/importer"
        },
        {
          "article": "Article 24(3)",
          "description": "Distributor must ensure storage/transport conditions do not jeopardise Section 2 compliance",
          "specific_condition": "While under their responsibility"
        },
        {
          "article": "Article 24(4)",
          "description": "If distributor considers non-conformity after making available, must take corrective actions or ensure others take them",
          "specific_condition": "If risk per Art 79(1), must immediately inform provider/importer and competent authorities with details"
        },
        {
          "article": "Article 24(5)",
          "description": "Distributor must provide information/documentation upon reasoned request",
          "specific_condition": "Regarding actions under paragraphs 1-4 to demonstrate conformity"
        },
        {
          "article": "Article 24(6)",
          "description": "Distributor must cooperate with authorities in actions regarding system",
          "specific_condition": "To reduce/mitigate risks"
        }
      ],
      "notes": "Distributors have verification and cooperation duties"
    },

    {
      "group_id": "WG-024",
      "group_name": "Value Chain Responsibility Shifts",
      "shared_requirements": [
        "Is distributor, importer, deployer or other third-party",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 25(1)(a)",
          "description": "Actor becomes provider if they put their name/trademark on already-placed/serviced high-risk system",
          "specific_condition": "Subject to all provider obligations under Art 16; without prejudice to contractual arrangements"
        },
        {
          "article": "Article 25(1)(b)",
          "description": "Actor becomes provider if they make substantial modification to already-placed/serviced high-risk system",
          "specific_condition": "Modification must be such that system remains high-risk per Art 6; subject to all provider obligations"
        },
        {
          "article": "Article 25(1)(c)",
          "description": "Actor becomes provider if they modify intended purpose of non-high-risk AI/general-purpose AI system",
          "specific_condition": "Modification must be such that system becomes high-risk per Art 6; subject to all provider obligations"
        },
        {
          "article": "Article 25(3)(a)",
          "description": "Product manufacturer becomes provider if high-risk AI system placed on market with product under manufacturer's name/trademark",
          "specific_condition": "For safety components of products in Annex I Section A; subject to Art 16 obligations"
        },
        {
          "article": "Article 25(3)(b)",
          "description": "Product manufacturer becomes provider if high-risk AI system put into service under manufacturer's name/trademark after product placed on market",
          "specific_condition": "For safety components of products in Annex I Section A; subject to Art 16 obligations"
        }
      ],
      "notes": "When responsibility shifts, initial provider no longer considered provider but must cooperate with new provider (Art 25(2)) unless clearly specified system not to be changed into high-risk. Written agreements required between providers and third-party suppliers (Art 25(4))."
    },

    {
      "group_id": "WG-025",
      "group_name": "Deployer Obligations - General",
      "shared_requirements": [
        "Is deployer of high-risk AI system (under Art 6)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 26(1)",
          "description": "Deployer must take appropriate technical and organisational measures to use system according to instructions",
          "specific_condition": "Per paragraphs 3 and 6"
        },
        {
          "article": "Article 26(2)",
          "description": "Deployer must assign human oversight to natural persons with necessary competence, training, authority and support",
          "specific_condition": "Human oversight requirement"
        },
        {
          "article": "Article 26(4)",
          "description": "Deployer exercising control over input data must ensure data is relevant and sufficiently representative",
          "specific_condition": "In view of intended purpose"
        },
        {
          "article": "Article 26(5)",
          "description": "Deployer must monitor operation based on instructions and inform providers of potential risks",
          "specific_condition": "If reason to consider use per instructions may present risk per Art 79(1), must inform provider/distributor/market surveillance authority without undue delay and suspend use. If serious incident, must immediately inform provider, then importer/distributor and market surveillance authorities. Exception: does not cover sensitive operational data of law enforcement deployers. Financial institutions may fulfil through Union financial services law compliance."
        },
        {
          "article": "Article 26(6)",
          "description": "Deployer must keep automatically generated logs under their control",
          "specific_condition": "At least 6 months unless otherwise provided; financial institutions may keep as part of financial services law documentation"
        },
        {
          "article": "Article 26(7)",
          "description": "Deployer who is employer must inform workers' representatives and affected workers before putting into service/using at workplace",
          "specific_condition": "Per Union/national law and practice on worker information"
        },
        {
          "article": "Article 26(8)",
          "description": "Deployers that are public authorities/Union institutions must comply with registration obligations",
          "specific_condition": "Per Art 49. If system not registered in EU database, must not use and must inform provider/distributor"
        },
        {
          "article": "Article 26(9)",
          "description": "Deployer must use Art 13 information to comply with data protection impact assessment",
          "specific_condition": "Where applicable, under Art 35 GDPR or Art 27 LED"
        },
        {
          "article": "Article 26(11)",
          "description": "Deployers of Annex III high-risk systems making/assisting decisions on natural persons must inform those persons",
          "specific_condition": "Inform that they are subject to use of high-risk AI system; for law enforcement, Art 13 LED applies"
        },
        {
          "article": "Article 26(12)",
          "description": "Deployer must cooperate with authorities in actions regarding system",
          "specific_condition": "To implement this Regulation"
        }
      ],
      "notes": "Comprehensive deployer obligations without prejudice to other Union/national law obligations"
    },

    {
      "group_id": "WG-026",
      "group_name": "Deployer Obligations - Post-Remote Biometric Identification",
      "shared_requirements": [
        "Is deployer of high-risk AI system for post-remote biometric identification",
        "For law enforcement purposes",
        "In framework of investigation for targeted search of suspected/convicted person",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 26(10)",
          "description": "Deployer must request authorisation from judicial/administrative authority for use",
          "specific_condition": "Ex ante or without undue delay within 48 hours; exception for initial identification based on objective verifiable facts. Each use limited to what strictly necessary for investigation of specific criminal offence. If rejected, use stopped immediately and personal data deleted. System cannot be used in untargeted way without link to offence/proceeding/threat/missing person. No adverse decision based solely on output. Each use documented and made available to market surveillance and data protection authorities. Deployers must submit annual reports. Member States may introduce more restrictive laws."
        }
      ],
      "notes": "Highly specific requirements for post-remote biometric identification, without prejudice to LED Art 9 and GDPR Art 9"
    },

    {
      "group_id": "WG-027",
      "group_name": "Deployer Obligations - Fundamental Rights Impact Assessment",
      "shared_requirements": [
        "Is deployer of high-risk AI system under Art 6(2)",
        "Deployer is public authority/body governed by public law OR private entity providing public services",
        "OR deployer is deploying Annex III point 5(b) or 5(c) system (credit/insurance)",
        "Except high-risk systems for Annex III point 2 (critical infrastructure)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 27(1)",
          "description": "Deployer must perform fundamental rights impact assessment prior to deployment",
          "specific_condition": "Assessment must include: (a) description of deployer's processes; (b) period and frequency of use; (c) categories of persons/groups affected; (d) specific risks of harm to those persons/groups; (e) human oversight implementation; (f) measures for risk materialisation including governance and complaints"
        },
        {
          "article": "Article 27(2)",
          "description": "Deployer may rely on previous assessments in similar cases",
          "specific_condition": "Must update if elements changed or no longer up to date"
        },
        {
          "article": "Article 27(3)",
          "description": "Deployer must notify market surveillance authority of assessment results",
          "specific_condition": "Submit filled-out template per Art 27(5); exemption possible per Art 46(1)"
        },
        {
          "article": "Article 27(4)",
          "description": "If DPIA under GDPR/LED already performed, fundamental rights impact assessment complements it",
          "specific_condition": "No duplication, complementary obligation"
        }
      ],
      "notes": "AI Office to develop template/automated tool per Art 27(5)"
    },

    {
      "group_id": "WG-028",
      "group_name": "Conformity Assessment - High-Risk AI Systems",
      "shared_requirements": [
        "Is provider of high-risk AI system (under Art 6)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 43(1) with Annex VII",
          "description": "For high-risk AI systems in Annex III except point 1, provider must follow conformity assessment based on internal control",
          "specific_condition": "Provider conducts self-assessment per Annex VII. Exception: biometric systems per Art 43(2) require notified body."
        },
        {
          "article": "Article 43(2)",
          "description": "For remote biometric identification systems (Annex III point 1) and high-risk systems in Annex I, provider must follow third-party conformity assessment",
          "specific_condition": "Involves notified body assessment per Annex VII or relevant Union harmonisation legislation"
        }
      ],
      "notes": "Different conformity assessment procedures depending on type of high-risk system"
    },

    {
      "group_id": "WG-029",
      "group_name": "CE Marking and Declaration of Conformity",
      "shared_requirements": [
        "Is provider of high-risk AI system (under Art 6)",
        "System assessed as conforming",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 47",
          "description": "Provider must draw up EU declaration of conformity per Annex V",
          "specific_condition": "Declaration attests system fulfils requirements; kept up to date"
        },
        {
          "article": "Article 48(1)",
          "description": "Provider must affix CE marking to indicate conformity",
          "specific_condition": "Physical marking for embedded systems, digital marking for digitally provided systems; must be visible, legible, indelible"
        }
      ],
      "notes": "CE marking is condition for placing on market/putting into service"
    },

    {
      "group_id": "WG-030",
      "group_name": "Registration Obligations - Providers",
      "shared_requirements": [
        "Is provider of high-risk AI system not related to products in Annex I",
        "OR provider considering Annex III system not high-risk per Art 6(4)",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 49(1)",
          "description": "Provider must register themselves and system information in EU database before placing on market/putting into service",
          "specific_condition": "Per Section A of Annex VIII"
        },
        {
          "article": "Article 49(2)",
          "description": "Provider considering Annex III system not high-risk must register in EU database",
          "specific_condition": "Before placing on market/putting into service; per derogation in Art 6(3)"
        },
        {
          "article": "Article 49(3)",
          "description": "Provider must register substantial modifications",
          "specific_condition": "When substantial modification occurs"
        },
        {
          "article": "Article 49(4)",
          "description": "For law enforcement/migration/asylum/border control systems, provider must register in secure non-public section",
          "specific_condition": "Access limited to Commission and market surveillance authorities"
        },
        {
          "article": "Article 49(5)",
          "description": "For critical infrastructure high-risk AI systems (Annex III point 2), registration only at national level",
          "specific_condition": "Not in EU database"
        }
      ],
      "notes": "Different registration pathways depending on system type"
    },

    {
      "group_id": "WG-031",
      "group_name": "Registration Obligations - Deployers",
      "shared_requirements": [
        "Is deployer of high-risk AI system listed in Annex III",
        "Deployer is public authority, agency, body, or Union institution",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 49(6)",
          "description": "Deployer must register themselves and select system in EU database before using",
          "specific_condition": "Per Section B of Annex VIII"
        },
        {
          "article": "Article 49(7)",
          "description": "For law enforcement/migration/asylum/border control systems, deployer must register in secure non-public section",
          "specific_condition": "If use is for those purposes"
        }
      ],
      "notes": "Other deployers entitled to register voluntarily. If system not registered, deployer must not use and must inform provider/distributor (Art 26(8))"
    },

    {
      "group_id": "WG-032",
      "group_name": "Transparency Obligations - AI System Interaction",
      "shared_requirements": [
        "Is provider/deployer of AI system intended to interact directly with natural persons",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 50(1)",
          "description": "Provider/deployer must inform natural persons they are interacting with AI system",
          "specific_condition": "Unless obvious from circumstances to reasonably well-informed, observant and circumspect person. Must consider characteristics of vulnerable groups when system intended for them. Information in accessible formats for persons with disabilities."
        }
      ],
      "notes": "Part of general transparency obligations"
    },

    {
      "group_id": "WG-033",
      "group_name": "Transparency Obligations - Emotion Recognition/Biometric Categorisation",
      "shared_requirements": [
        "Is provider/deployer of emotion recognition system OR biometric categorisation system",
        "System processes biometric data",
        "System not prohibited under Art 5",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 50(2)",
          "description": "Provider/deployer must inform natural persons they are exposed to system",
          "specific_condition": "System identifies/infers emotions/intentions or assigns to specific categories. Information in accessible formats for persons with disabilities."
        }
      ],
      "notes": "Applies to non-prohibited emotion recognition and biometric categorisation systems"
    },

    {
      "group_id": "WG-034",
      "group_name": "Transparency Obligations - Content Detection/Labelling",
      "shared_requirements": [
        "Is provider of AI system generating synthetic audio/image/video/text content",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 50(3)",
          "description": "Provider must ensure system is designed/developed to enable detection that output is artificially generated/manipulated",
          "specific_condition": "Through machine-readable marking in effective, interoperable, robust manner (e.g., watermarks, metadata, cryptographic methods). Must consider state of art. Not applicable if system performs assistive function for standard editing or does not substantially alter input data/semantics."
        }
      ],
      "notes": "Technical solution may be at AI system or AI model level"
    },

    {
      "group_id": "WG-035",
      "group_name": "Transparency Obligations - Deep Fake Disclosure",
      "shared_requirements": [
        "Is deployer using AI system to generate/manipulate image/audio/video content (deep fake)",
        "Content appreciably resembles existing persons/objects/places/entities/events",
        "Would falsely appear authentic to a person",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 50(4)",
          "description": "Deployer must disclose that content artificially created/manipulated by labelling output and disclosing artificial origin",
          "specific_condition": "Disclosure must be clear and distinguishable. Limited obligation for creative/satirical/artistic/fictional works - only disclosure of existence in appropriate manner not hampering display/enjoyment. Similar obligation for AI-generated text published for informing public, unless undergone human review/editorial control with editorial responsibility."
        }
      ],
      "notes": "Does not impede freedom of expression or arts and sciences rights per Charter"
    },

    {
      "group_id": "WG-036",
      "group_name": "General-Purpose AI Model - Classification",
      "shared_requirements": [
        "Is general-purpose AI model",
        "Model meets threshold of at least 10^25 FLOPs for training",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 51(1)(a)",
          "description": "Model is presumed to be general-purpose AI model with systemic risk",
          "specific_condition": "Based on cumulative computation threshold. Provider must notify Commission within 2 weeks (Art 52(1)). Provider may present arguments that model does not present systemic risk (Art 52(2))."
        },
        {
          "article": "Article 51(1)(b)",
          "description": "Model is classified as general-purpose AI model with systemic risk based on Commission decision",
          "specific_condition": "Commission designates based on Annex XIII criteria (e.g., number of users, model capabilities, impact on internal market) per Art 52(4)"
        }
      ],
      "notes": "Commission to adjust threshold over time. Scientific panel may issue qualified alerts (Art 90, 52(4)). Providers may request reassessment (Art 52(5))."
    },

    {
      "group_id": "WG-037",
      "group_name": "General-Purpose AI Model Provider Obligations - Basic",
      "shared_requirements": [
        "Is provider of general-purpose AI model",
        "Model placed on Union market",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 53(1)(a)",
          "description": "Provider must draw up and keep up-to-date technical documentation per Annex XI",
          "specific_condition": "For provision to AI Office and national competent authorities upon request"
        },
        {
          "article": "Article 53(1)(b)",
          "description": "Provider must provide information and documentation downstream to enable AI system provider compliance",
          "specific_condition": "Must contain at minimum elements in Annex XII to enable understanding of model capabilities/limitations"
        },
        {
          "article": "Article 53(1)(c)",
          "description": "Provider must put in place policy to comply with Union copyright law",
          "specific_condition": "Including identifying and complying with reservation of rights per Art 4(3) Directive 2019/790 using state-of-art technologies"
        },
        {
          "article": "Article 53(1)(d)",
          "description": "Provider must draw up and make publicly available detailed summary of training content",
          "specific_condition": "Per template provided by AI Office"
        },
        {
          "article": "Article 53(3)",
          "description": "Provider must cooperate with Commission and national competent authorities",
          "specific_condition": "As necessary in exercise of their competences and powers"
        }
      ],
      "notes": "Exemptions for free and open-source models under Art 53(2) except for systemic risk models. Copyright obligations (c) and (d) apply even to FOSS models (per Recital 104). Providers may rely on codes of practice (Art 53(4))."
    },

    {
      "group_id": "WG-038",
      "group_name": "General-Purpose AI Model Provider Obligations - Systemic Risk",
      "shared_requirements": [
        "Is provider of general-purpose AI model with systemic risk (per Art 51)",
        "Model placed on Union market",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 55(1)(a)",
          "description": "Provider must perform model evaluation per standardised protocols including adversarial testing",
          "specific_condition": "With view to identifying and mitigating systemic risks; documenting evaluation"
        },
        {
          "article": "Article 55(1)(b)",
          "description": "Provider must assess and mitigate possible systemic risks at Union level",
          "specific_condition": "Including their sources from development, placing on market, or use"
        },
        {
          "article": "Article 55(1)(c)",
          "description": "Provider must keep track of, document, and report serious incidents to AI Office",
          "specific_condition": "Without undue delay, report to AI Office and as appropriate to national competent authorities; include possible corrective measures"
        },
        {
          "article": "Article 55(1)(d)",
          "description": "Provider must ensure adequate level of cybersecurity protection for model and physical infrastructure",
          "specific_condition": "Protection throughout lifecycle"
        }
      ],
      "notes": "These are in addition to obligations in Art 53 and 54. Providers may rely on codes of practice (Art 55(2))."
    },

    {
      "group_id": "WG-039",
      "group_name": "General-Purpose AI Model - Authorised Representative",
      "shared_requirements": [
        "Is provider of general-purpose AI model established in third country",
        "Intends to place model on Union market",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 54(1)",
          "description": "Provider must appoint authorised representative established in Union by written mandate",
          "specific_condition": "Prior to placing model on Union market"
        },
        {
          "article": "Article 54(3)",
          "description": "Authorised representative must perform specified tasks",
          "specific_condition": "Tasks include: (a) verifying technical documentation and obligations fulfilled; (b) keeping documentation for 10 years; (c) providing information to AI Office; (d) cooperating with AI Office and authorities"
        },
        {
          "article": "Article 54(5)",
          "description": "Authorised representative must terminate mandate if provider acts contrary to obligations",
          "specific_condition": "Must immediately inform AI Office of termination and reasons"
        }
      ],
      "notes": "Exemption for FOSS models under Art 54(6) unless systemic risk"
    },

    {
      "group_id": "WG-040",
      "group_name": "AI Regulatory Sandbox - Establishment",
      "shared_requirements": [
        "Is Member State"
      ],
      "legal_consequences": [
        {
          "article": "Article 57(1)",
          "description": "Member State must ensure competent authorities establish at least one AI regulatory sandbox at national level",
          "specific_condition": "Operational by 2 August 2026. May be established jointly with other Member States or fulfilled by participating in existing sandbox."
        },
        {
          "article": "Article 57(4)",
          "description": "Member State must ensure competent authorities allocate sufficient resources for sandbox",
          "specific_condition": "Financial and human resources; cooperation with other relevant authorities"
        }
      ],
      "notes": "Sandboxes provide controlled environment for innovation, development, testing and validation before market placement. Detailed arrangements in Art 58."
    },

    {
      "group_id": "WG-041",
      "group_name": "Testing in Real World Conditions - Requirements",
      "shared_requirements": [
        "Is provider/prospective provider of high-risk AI system listed in Annex III",
        "Intends to conduct testing in real world conditions outside sandbox",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 60(4)",
          "description": "Provider may conduct testing only if all 11 conditions met",
          "specific_condition": "Must: (a) draw up and submit real-world testing plan to market surveillance authority; (b) obtain approval within 30 days; (c) register testing in EU database; (d) be established in Union or appoint legal representative; (e) ensure appropriate data transfer safeguards; (f) limit testing to max 6+6 months; (g) protect vulnerable groups; (h) inform deployers and conclude agreement; (i) obtain informed consent (or exception for law enforcement); (j) ensure effective oversight; (k) ensure predictions/decisions reversible"
        },
        {
          "article": "Article 60(7)",
          "description": "Provider must report serious incidents to market surveillance authority and adopt mitigation or suspend/terminate testing",
          "specific_condition": "Per Art 73; must establish recall procedure"
        },
        {
          "article": "Article 60(8)",
          "description": "Provider must notify market surveillance authority of suspension/termination and final outcomes",
          "specific_condition": "Notification obligation"
        },
        {
          "article": "Article 60(9)",
          "description": "Provider liable for damage caused during testing",
          "specific_condition": "Under applicable Union and national liability law"
        }
      ],
      "notes": "Informed consent requirements detailed in Art 61. Subjects may withdraw consent at any time (Art 60(5))."
    },

    {
      "group_id": "WG-042",
      "group_name": "Post-Market Monitoring",
      "shared_requirements": [
        "Is provider of high-risk AI system (under Art 6)",
        "System placed on market or put into service",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 72",
          "description": "Provider must establish and maintain post-market monitoring system",
          "specific_condition": "Actively and systematically collect, document and analyse data on performance throughout lifecycle; based on post-market monitoring plan; proportionate to nature of AI technology and risks"
        }
      ],
      "notes": "Part of quality management system obligation. Does not cover sensitive operational data of law enforcement deployers."
    },

    {
      "group_id": "WG-043",
      "group_name": "Serious Incident Reporting",
      "shared_requirements": [
        "Is provider of high-risk AI system (under Art 6)",
        "Serious incident occurs",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 73(1)",
          "description": "Provider must report serious incident to market surveillance authorities of Member States where incident occurred",
          "specific_condition": "Immediately after establishing causal link or reasonable likelihood of link. Report must be followed by detailed report per Art 73(2)."
        },
        {
          "article": "Article 73(2)",
          "description": "Provider must provide detailed report within 15 days of becoming aware",
          "specific_condition": "If causal link not yet established, provisional report followed by final report when information available"
        }
      ],
      "notes": "Serious incident means incident/malfunctioning leading to death/serious health damage, serious/irreversible critical infrastructure disruption, fundamental rights violations, or serious damage to property/environment. Deployers must also report if unable to reach provider (Art 26(5))."
    },

    {
      "group_id": "WG-044",
      "group_name": "Member State Governance Obligations",
      "shared_requirements": [
        "Is Member State"
      ],
      "legal_consequences": [
        {
          "article": "Article 70(1)",
          "description": "Member State must designate at least one notifying authority responsible for conformity assessment bodies",
          "specific_condition": "Per Art 28"
        },
        {
          "article": "Article 70(2)",
          "description": "Member State must designate at least one market surveillance authority",
          "specific_condition": "As national competent authority for supervising application and implementation"
        },
        {
          "article": "Article 70(3)",
          "description": "Member State must designate one of the market surveillance authorities as single point of contact",
          "specific_condition": "Vis--vis public, operators and Commission"
        },
        {
          "article": "Article 70(7)",
          "description": "Member States must facilitate SME access and provide support measures",
          "specific_condition": "Per Art 62, including awareness raising, priority sandbox access, communication channels"
        }
      ],
      "notes": "Member States may designate any public entity. Authorities must exercise powers independently, impartially, without bias."
    },

    {
      "group_id": "WG-045",
      "group_name": "Penalties and Enforcement",
      "shared_requirements": [
        "Infringement of this Regulation has occurred",
        "In scope of Regulation"
      ],
      "legal_consequences": [
        {
          "article": "Article 99(3)",
          "description": "Infringement of Art 5 prohibitions subject to administrative fines up to 35,000,000 or 7% of total worldwide annual turnover",
          "specific_condition": "Whichever is higher"
        },
        {
          "article": "Article 99(4)",
          "description": "Infringement of Art 6-9, 11-15, 19-26, 28-30, 72-74 operator obligations subject to fines up to 15,000,000 or 3% of total worldwide annual turnover",
          "specific_condition": "Whichever is higher"
        },
        {
          "article": "Article 99(5)",
          "description": "Supply of incorrect/incomplete/misleading information to authorities subject to fines up to 7,500,000 or 1% of total worldwide annual turnover",
          "specific_condition": "Whichever is higher; or up to 1,500,000 for SMEs"
        },
        {
          "article": "Article 101(3)",
          "description": "Non-compliance by general-purpose AI model providers with Arts 53, 54, 55 subject to fines up to 15,000,000 or 3% of total worldwide annual turnover",
          "specific_condition": "Whichever is higher; or up to 3% of turnover or 7,500,000 for SMEs. Non-compliance with Commission requests per Art 91 subject to fines up to 7,500,000 or 1% of turnover, or up to 1% or 1,500,000 for SMEs."
        }
      ],
      "notes": "Member States must lay down rules on penalties ensuring they are effective, proportionate and dissuasive. SMEs and start-ups have proportionately lower maximum fines. EDPS has power to impose fines on Union institutions, bodies, offices and agencies."
    }
  ],

  "ungrouped_wirknormen": [
    {
      "article": "Article 8(2)",
      "description": "For products containing AI systems covered by this Regulation and Union harmonisation legislation in Annex I Section A, providers may integrate compliance processes/documentation to avoid duplication",
      "requirements": [
        "Is provider of high-risk AI system",
        "Product also subject to Union harmonisation legislation in Annex I Section A"
      ],
      "reason_ungrouped": "Procedural integration option, not a distinct legal consequence requiring separate grouping"
    },
    {
      "article": "Article 46",
      "description": "Under exceptional circumstances, market surveillance authorities may authorise placing on market/putting into service of AI systems not having undergone conformity assessment",
      "requirements": [
        "Exceptional reasons of public security or protection of life/health/environment/critical infrastructure",
        "Authority authorises",
        "Limited time period"
      ],
      "reason_ungrouped": "Exceptional derogation with highly specific procedural requirements not fitting standard grouping pattern"
    },
    {
      "article": "Article 59",
      "description": "In AI regulatory sandbox, personal data lawfully collected for other purposes may be processed for developing/training/testing certain AI systems in public interest",
      "requirements": [
        "In AI regulatory sandbox",
        "AI systems in public interest areas (public safety/health, environment, energy, transport, public administration)",
        "10 cumulative conditions met per Art 59(1)"
      ],
      "reason_ungrouped": "Highly specific data processing permission for sandbox context only, with complex cumulative conditions"
    },
    {
      "article": "Article 85(2)",
      "description": "Affected persons have right to obtain explanation from deployer where decision based on output of certain high-risk AI systems and produces legal effects or similarly significantly affects them",
      "requirements": [
        "Decision based mainly on output from high-risk AI system",
        "Decision produces legal effects or similarly significantly affects person",
        "Person considers adverse impact on health, safety or fundamental rights",
        "System falls within scope of Regulation",
        "No exception/restriction under Union/national law"
      ],
      "reason_ungrouped": "Individual rights provision rather than operator obligation; unique requirements structure"
    },
    {
      "article": "Article 7(1)",
      "description": "Commission empowered to adopt delegated acts amending Annex III by adding/modifying high-risk use-cases",
      "requirements": [
        "AI systems in areas listed in Annex III",
        "Risk equivalent to or greater than existing Annex III systems",
        "Based on 11 criteria in Art 7(2)"
      ],
      "reason_ungrouped": "Regulatory power rather than immediate legal consequence; dynamic amendment mechanism"
    },
    {
      "article": "Article 6(6)-(7)",
      "description": "Commission empowered to adopt delegated acts amending conditions in Art 6(3) second subparagraph (derogation from high-risk classification)",
      "requirements": [
        "Concrete and reliable evidence for adding/deleting conditions"
      ],
      "reason_ungrouped": "Regulatory power for amending derogation conditions"
    },
    {
      "article": "Articles 64-69",
      "description": "Governance structure provisions establishing AI Office, Board, scientific panel, advisory forum",
      "requirements": [
        "Governance provisions"
      ],
      "reason_ungrouped": "Structural/institutional provisions rather than operator obligations or direct legal consequences"
    }
  ],

  "analysis_notes": {
    "key_observations": [
      "The EU AI Act creates a multi-tiered system of legal consequences based on risk level: (1) Prohibited practices; (2) High-risk AI systems with comprehensive requirements; (3) Limited transparency obligations for certain AI systems; (4) General-purpose AI models with basic and enhanced (systemic risk) obligations.",

      "Article 5 prohibitions share a base structure (AI system + placing on market/service/use + in scope) but each has unique trigger conditions. Real-time remote biometric identification (Art 5(1)(h)) is not an absolute prohibition but a heavily restricted permission requiring multiple cumulative conditions.",

      "High-risk classification follows two pathways: (1) Safety components of products requiring third-party conformity assessment under Annex I legislation (Art 6(1)); (2) AI systems listed in Annex III use-cases (Art 6(2)), with derogation possibility under Art 6(3) if system does not materially influence outcome.",

      "Annex III creates 27 specific high-risk AI system consequences across 8 areas: (1) Biometrics; (2) Critical infrastructure; (3) Education; (4) Employment; (5) Essential services; (6) Law enforcement; (7) Migration/asylum/border control; (8) Administration of justice and democracy. Each has slightly different specific conditions.",

      "High-risk AI system requirements (Articles 9-15) apply cumulatively and must be implemented through the risk management system. These include: risk management, data governance, technical documentation, logging, transparency/instructions, human oversight, and accuracy/robustness/cybersecurity.",

      "Provider obligations (Article 16) create 12 distinct duties. Some obligations have conditional applicability (e.g., financial institutions may fulfill quality management through existing financial services law compliance - Art 17(4)).",

      "Value chain responsibility is complex: actors can become providers by putting their name/trademark on systems (Art 25(1)(a)), making substantial modifications (Art 25(1)(b)), or changing intended purpose to make system high-risk (Art 25(1)(c)). Product manufacturers become providers for safety component AI systems under specific conditions (Art 25(3)).",

      "Deployer obligations vary significantly based on: (1) General requirements (Art 26(1)-(12)); (2) Additional requirements for public authorities/Union institutions (registration - Art 26(8)); (3) Additional requirements for certain deployers (fundamental rights impact assessment - Art 27); (4) Specific requirements for post-remote biometric identification (Art 26(10)).",

      "Transparency obligations (Article 50) create 4 distinct consequence types: (1) Notification of AI interaction (Art 50(1)); (2) Notification of emotion recognition/biometric categorisation (Art 50(2)); (3) Technical detection enablement by providers (Art 50(3)); (4) Deep fake disclosure by deployers (Art 50(4)).",

      "General-purpose AI models have two obligation tiers: (1) Basic obligations for all models (Art 53) - documentation, downstream information, copyright policy, training summary; (2) Additional obligations for systemic risk models (Art 55) - model evaluation, systemic risk assessment/mitigation, incident reporting, cybersecurity. Free and open-source models have partial exemption (Art 53(2)) except for systemic risk models and except copyright/training summary obligations (Art 53(1)(c)-(d)).",

      "Registration obligations split into multiple categories: (1) Providers of standalone high-risk AI systems (Art 49(1)); (2) Providers claiming derogation from high-risk (Art 49(2)); (3) Law enforcement/migration/asylum/border control systems in secure section (Art 49(4)); (4) Critical infrastructure systems at national level only (Art 49(5)); (5) Public authority/Union institution deployers (Art 49(6)).",

      "Penalties are stratified by severity: Highest fines (35M or 7% turnover) for Art 5 prohibition violations; 15M or 3% for high-risk system requirement violations; 7.5M or 1% for incorrect information; proportionately lower for SMEs. Commission has enforcement power over general-purpose AI models; Member State market surveillance authorities over other AI systems."
    ],

    "grouping_challenges": [
      "Article 5(1)(h) (real-time remote biometric identification) presents grouping challenges because each of the three permitted use-cases (victim search, imminent threat, serious crime) shares identical procedural requirements (prior authorisation, fundamental rights impact assessment, proportionality conditions, registration, notification, annual reporting) but has different substantive trigger conditions. Grouped together as they share all requirements except the specific public interest objective.",

      "Annex III high-risk AI systems (27 distinct use-cases) could theoretically be grouped into fewer categories, but the specific conditions for each differ meaningfully. For instance, 'education access/admission' (point 3(a)), 'learning outcome evaluation' (point 3(b)), 'education level assessment' (point 3(c)), and 'student monitoring' (point 3(d)) all concern education but have distinct purposes and implications, warranting separate grouping.",

      "Article 25 (value chain responsibility shifts) creates multiple pathways for actors to become providers, but each pathway has unique conditions and triggers, preventing consolidation into a single group.",

      "Article 26 deployer obligations include both universal duties (Art 26(1)-(12)) and conditional duties based on deployer type (public authority registration, FRIA requirement) or system type (post-remote biometric identification). These are grouped separately due to different requirement profiles.",

      "General-purpose AI model obligations (Arts 53, 55) apply cumulatively for systemic risk models, but because basic obligations apply to all models while systemic risk obligations apply only to subset, they are grouped separately.",

      "Many obligations contain embedded exceptions or qualifications (e.g., financial institutions may fulfill certain obligations through existing compliance regimes - Art 17(4), 26(5), 26(6)). These are noted within groups rather than creating separate groups because the core obligation structure remains the same."
    ],

    "structural_patterns": [
      "Prohibitions (Art 5) follow pattern: Base requirements (AI system + in scope + placing on market/service/use) + Specific prohibited conduct + Harm threshold/effect",

      "High-risk classification follows: AI system + One of two pathways (safety component route or Annex III route) + Not excluded by derogation",

      "High-risk system requirements follow: Is high-risk AI system + Specific requirement (risk management, data governance, etc.) + Must be implemented throughout lifecycle",

      "Operator obligations follow: Actor type (provider/deployer/importer/distributor) + High-risk AI system + Specific obligation + Conditions/timeframes",

      "Transparency obligations follow: Actor type + AI system type + Triggering circumstance + Disclosure/design requirement",

      "General-purpose AI model obligations follow: Provider + Model type (all models vs. systemic risk) + Specific obligation + May rely on codes of practice",

      "Most obligations contain temporal dimensions (before placing on market, throughout lifecycle, within X days/months, for 10 years, etc.) that are essential to the Wirknorm",

      "Many provisions explicitly state 'without prejudice to' other Union/national law, indicating these are complementary rather than exhaustive obligations"
    ],

    "cross-cutting_requirements": [
      "Scope limitation (Article 2) is a universal requirement - all Wirknormen apply only 'in scope of Regulation'. Exclusions include: military/defence/national security (Art 2(3)), purely personal non-professional activity (Art 2(4)), systems released before being placed on market solely for research/development/prototyping (Art 2(6))",

      "Definitions (Article 3) are foundational - whether something is an 'AI system', 'provider', 'deployer', 'placing on market', 'putting into service', 'high-risk AI system', 'general-purpose AI model' determines applicability of most Wirknormen",

      "Temporal provisions (Article 113) affect applicability: Most provisions apply from 2 August 2026; prohibitions from 2 February 2025; general-purpose AI model obligations from 2 August 2025; governance provisions from 2 August 2025",

      "Grandfathering (Article 111) affects existing systems: Generally not retroactive, except significant changes after 2 August 2026 trigger compliance. Public authority systems must comply by 2 August 2030 (high-risk standalone) or end of 2030 (large-scale IT systems)",

      "Confidentiality (Article 78) applies across board - all information obtained by authorities must be treated in accordance with confidentiality obligations, protecting IP, trade secrets, confidential business information",

      "Small and medium enterprises have various facilitations: priority sandbox access (Art 62(1)(a)), simplified technical documentation form (Art 11(1)), simplified quality management (Art 63(1)), proportionate conformity assessment fees (Art 62(2)), lower penalty caps (Arts 99(5), 101(3))"
    ],

    "edge_cases_and_uncertainties": [
      "Article 6(3) derogation from high-risk classification creates uncertainty: 'materially influencing the outcome' and 'not posing significant risk' are inherently fact-specific determinations. The four conditions (narrow procedural task, improvement of prior activity, pattern detection, preparatory task) require interpretation. Exception to exception: profiling always remains high-risk, but 'profiling' must be interpreted per GDPR Art 4(4), LED Art 3(4), EUDPR Art 3(5).",

      "Article 5(1)(h) 'substantial and imminent threat' and 'genuine and present or genuine and foreseeable threat of terrorist attack' create interpretive challenges. 'Imminent' typically means immediate, but 'foreseeable' may allow longer timeframes. Case law will be necessary.",

      "General-purpose AI model classification threshold (10^25 FLOPs) is technical and may require expert determination. Commission may adjust over time (Art 51(2)). Systemic risk designation based on Annex XIII criteria involves judgment calls.",

      "Intersection between this Regulation and Union financial services law, data protection law, consumer protection law, sectoral product safety legislation creates complex compliance matrices. Multiple legal bases may apply concurrently (as clarified in Recital 64, 158, 168).",

      "Free and open-source license exception (Art 53(2), 54(6)) applies to general-purpose AI models, but parameters must be 'made publicly available' including 'weights, information on model architecture, and information on model usage.' Determining whether release qualifies requires technical assessment. Exception does not apply to systemic risk models or to copyright policy/training summary obligations.",

      "Article 25 substantial modification determining when actor becomes provider is fact-intensive. Must assess whether modification is 'substantial' and whether modified system 'remains high-risk' or 'becomes high-risk.' No bright-line rule.",

      "Article 27 fundamental rights impact assessment scope for 'private entities providing public services' may generate disputes. What constitutes 'public service' provided by private entity is not defined and may vary by Member State."
    ]
  }
}
